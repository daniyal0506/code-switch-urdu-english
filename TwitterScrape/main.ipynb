{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.corpus\n",
    "import nltk.stem\n",
    "import string\n",
    "\n",
    "from nltk.corpus import words as nltk_words\n",
    "from ipynb.fs.full.langdet import score_checker\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_contractions(phrase):\n",
    "\n",
    "     # remove contractions such as \"they've\" to \"they have\"\n",
    "\n",
    "     phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "     return phrase\n",
    "\n",
    "def remove_content(text):\n",
    "\n",
    "     # remove non-alphabet characthers, white spaces and links\n",
    "\n",
    "     text = text.lower()\n",
    "     text = text.encode('ascii', 'ignore').decode()\n",
    "     text = re.sub(\"@\\S+\", \" \", text)\n",
    "     text = re.sub(\"https*\\S+\", \" \", text)\n",
    "     text = re.sub(\"#\\S+\", \" \", text)\n",
    "     text = remove_contractions(text)\n",
    "     text = re.sub(\"\\'\\w+\", '', text)\n",
    "     text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "     text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
    "     text = re.sub('\\s{2,}', \" \", text)\n",
    "     \n",
    "     return text\n",
    "\n",
    "def is_english_word(word):\n",
    "\n",
    "     # creates a dictionary using the nltk words corpus\n",
    "    dictionary = dict.fromkeys(nltk_words.words(), None)\n",
    "\n",
    "     # checks to see if the word parameter is apart of this dictionary and then returns a value\n",
    "    try:\n",
    "        x = dictionary[word]\n",
    "        return True\n",
    "    except KeyError:\n",
    "        return False\n",
    "\n",
    "def confirm_cs(text):\n",
    "\n",
    "     # bool values to state what languages are present in a tweet\n",
    "     is_urdu = False\n",
    "     is_eng = False\n",
    "\n",
    "     # tokenize the text\n",
    "     tokenized_text = word_tokenize(text, language=\"english\") \n",
    "\n",
    "     for item in tokenized_text:\n",
    "\n",
    "          # confirm wether the word is urdu or english based of the language model and the english dictionary\n",
    "          if((score_checker(item) > 12.389025170796918) and (is_english_word(item))):\n",
    "               is_eng = True\n",
    "          elif ((score_checker(item) < 12.389025170796918) and (not(is_english_word(item)))):\n",
    "               is_urdu = True\n",
    "          \n",
    "          if (is_eng and is_urdu):\n",
    "               break\n",
    "     \n",
    "     #if both are true, there is code-switching occuring\n",
    "     if (is_eng and is_urdu):\n",
    "          return True\n",
    "     else:\n",
    "          return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "twitter_pakistan_collection=[]\n",
    "\n",
    "# scrapes twitter data and narrows the region to Pakistan\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('near:\"pakistan\"').get_items()):\n",
    "    \n",
    "    # will stop once 500 tweets are scraped\n",
    "    if len(twitter_pakistan_collection)> 500:\n",
    "        break\n",
    "\n",
    "    # confirms that the tweets are written in the roman script\n",
    "    if (tweet.lang =='en'):\n",
    "\n",
    "        #cleans the tweet before it is appended to data list\n",
    "        tweet.content = remove_content(tweet.content)\n",
    "\n",
    "        loc = 'pakistan'\n",
    "        if (loc in (tweet.user.location).lower()):\n",
    "            twitter_pakistan_collection.append([tweet.id, tweet.content]) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a pandas data frame to display the tweets prior to filtering\n",
    "tweets_Pakistan = pd.DataFrame(twitter_pakistan_collection, columns=['Tweet Id', 'Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores the tweets in a csv file so that they can be used later for further filtering\n",
    "tweets_Pakistan.to_csv(\"TwitterData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('TwitterData.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "del data[0]\n",
    "\n",
    "# removes ID and tweet number\n",
    "for item in data:\n",
    "    del item[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemixed_data = []\n",
    "\n",
    "for item in data:\n",
    "\n",
    "    #iterates through the tweets and confirms wether or not there is a code-mixing in a tweet\n",
    "    if (confirm_cs(item[0])):\n",
    "        codemixed_data.append(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codemixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text):\n",
    "\n",
    "     words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "     \n",
    "     return [word for word in words ]\n",
    "\n",
    "words = split_text(str(codemixed_data))\n",
    "\n",
    "# displays bigrams for the code-mixed rich tweets\n",
    "ngram_analysis = pd.Series(nltk.ngrams(words, 2)).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99c1a4d8383395b61034d24e6ba7046bc654bf940ed531e290ff2483e6d4138b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
